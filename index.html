    <!DOCTYPE HTML>
    <html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
      <style>
          
        /* .image-container {
            position: relative;
            display: inline-block;
        } */
          
        .image-container img {
            transition: transform 0.3s ease;
        }
    
        .image-container:hover img {
            transform: scale(1.5);
        }
    a {
        color: rgb(79, 136, 204); 
    }

          
    </style>
    <title><strong>Lulu Tang </strong></title>
  </head>

  <body>
    <table width="1000" border="0" align="center" cellspacing="0" cellpadding="0">
      <tr>
        <td>
          <!-- Header and Personal Info -->
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td width="67%" valign="middle">
                <p align="center">
                  <name>Lulu Tang</name>
                </p>
                <p>
                  I am a researcher at Beijing Academy of Artificial Intelligence <a href="https://www.baai.ac.cn/english.html/">(BAAI)</a>. 
                  I completed my postdoctoral research at Department of Automation, Tsinghua University, under the supervision of Prof.<a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>. I previously received my Ph.D. from University of Macao, advised by Prof. Zhi-xin Yang. Before that, and  my M.S. from Sichuan University, supervised by Prof. Qichan Zhang.
                </p>
                <p>
                  My research interests lie in the area of 3D computer vision, 3D generation models, and Vision-Language foundation models.
                </p>
                <p align="center">
                  <a href="mailto:lulutang978@gmail.com">Email</a> &nbsp|&nbsp
                  <a href="https://github.com/Julie-tang00">GitHub</a> &nbsp|&nbsp
                  <a href="https://scholar.google.com/citations?hl=en&user=o2fG4xUAAAAJ">Google Scholar</a>
                </p>
              </td>
              <td width="33%">
                <img src="image/tanglulu.jpg" width="250" alt="headshot">
              </td>
            </tr>
          </table>

          <!-- Research Section -->
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td width="100%" valign="middle">
                <heading>Selected Publications [todo]</heading>
                <p>(*: Equal Contribution, &clubs;: corresponding author)</p>
              </td>
            </tr>
          </table>

          <!-- Research Paper -->
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="image/pointbert.png" alt="PointBERT">
              </td>
              <td width="75%" valign="center">
                <papertitle>Point-BERT: Pre-Training 3D Point Cloud Transformers with Masked Point Modeling</papertitle>
                <br>
                <a href="https://yuxumin.github.io/">Xumin Yu</a>*,
                <a href="https://julie-tang00.github.io/"><strong>Lulu Tang</strong></a>*,
                <a href="https://raoyongming.github.io/">Yongming Rao</a>*,
                <a href="http://www.ai.pku.edu.cn/info/1139/1243.htm">Tiejun Huang</a>,
                <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>,
                <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>&clubs;
                <br>
                <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022
                <br>
                <a href="https://arxiv.org/abs/2111.14819">[arXiv]</a>
                <a href="https://github.com/lulutang0608/Point-BERT">[Code]</a>
                <a href="https://point-bert.ivg-research.xyz/">[Project Page]</a>
                <a href="https://zhuanlan.zhihu.com/p/484336830">[中文解读]</a>
                <br>
                <p>Point-BERT is a new paradigm for learning Transformers in an unsupervised manner by generalizing the concept of BERT onto 3D point cloud data.</p>
              </td>
            </tr>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="image/TAP.png" alt="TAP">
              </td>
              <td width="75%" valign="center">
                <papertitle>TAP: Tokenize Anything via Prompting</papertitle>
                <br>
                <a href="https://github.com/PhyscalX/">Ting Pan</a>*,
                <a href="https://julie-tang00.github.io/"><strong>Lulu Tang</strong></a>*,
                <a href="https://www.xloong.wang/">Xinlong Wang</a>&clubs;,
                <a href="https://scholar.google.com/citations?user=Vkzd7MIAAAAJ&hl=en">Shiguang Shan</a>,
                <br>
                <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2024
                <br>
                <a href="https://arxiv.org/pdf/2312.09128">[arXiv]</a>
                <a href="https://github.com/baaivision/tokenize-anything?tab=readme-ov-file">[Code]</a>
                <a href="https://huggingface.co/spaces/BAAI/tokenize-anything">[Demo]</a>
                <br>
                <p>TAP is a unified and promptable model capable of simultaneously segmenting, recognizing, and captioning arbitrary regions, with flexible visual prompts (point, box and sketch).</p>
              </td>
            </tr>
         
          </table>
                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="image/see3d.png" alt="see3d">
              </td>
              <td width="75%" valign="center">
                <papertitle>You See it, You Got it: Learning 3D Creation on Pose-Free Videos at Scale</papertitle>
                <br>
                <a href="https://mabaorui.github.io/">Baorui Ma</a>*,
                <a href="https://gaohchen.github.io/">Huachen Gao</a>*,
                <a href="https://github.com/Bitterdhg/">Haoge Deng</a>*,
                <a href="https://greatlog.github.io/">Zhengxiong Luo</a>,
                <a href="https://scholar.google.com/citations?user=knvEK4AAAAAJ&hl=en">Tiejun Huang</a>,
                <a href="https://julie-tang00.github.io/"> <strong>Lulu Tang</strong></a>&clubs;,
                <a href="https://www.xloong.wang/">Xinlong Wang</a>&clubs;,
                <br>
                <br>
                <a href="https://arxiv.org/abs/2412.06699">[arxiv]</a>
                <a href="https://vision.baai.ac.cn/see3d/">[project page]</a>
                <a href="https://github.com/baaivision/See3D">[code]</a>
                <a href="https://github.com/baaivision/See3D">[dataset]</a>
                <a href="https://mp.weixin.qq.com/s/tP_YOkL6kAdeoHf-44Ls5Q">[post]</a>
                <br>
                <p>See3D is a scalable visual-conditional MVD model for open-world 3D creation, which can be trained on web-scale video collections without camera pose annotations.</p>
              </td>
            </tr>
          </table>
          
        </td>
      </tr>
    </table>
  </body>
</html>
