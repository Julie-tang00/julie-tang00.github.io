<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <style>
      
    /* .image-container {
        position: relative;
        display: inline-block;
    } */
      
    .image-container img {
        transition: transform 0.3s ease;
    }

    .image-container:hover img {
        transform: scale(1.5);
    }
a {
    color: rgb(79, 136, 204); 
}

      
</style>
<title>Lulu Tang</title>
</head>

<body>
<table width="1000" border="0" align="center" cellspacing="0" cellpadding="0">
  <tr>
    <td>
      <!-- Header and Personal Info -->
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="67%" valign="middle">
            <p align="center">
              <name><strong>Lulu Tang</strong></name>
            </p>
            <p>
              I am a researcher at Beijing Academy of Artificial Intelligence <a href="https://www.baai.ac.cn/english.html/">(BAAI)</a>. 
              I completed my postdoctoral research at BAAI and Tsinghua University, under the supervision of Prof.  <a href="http://www.ai.pku.edu.cn/info/1139/1243.htm">Tiejun Huang</a> and Prof. <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu</a>. I previously received my Ph.D. degree from University of Macao, and M.S. degree from Sichuan University.
            </p>
            <p>
              My research interests lie in the area of 3D computer vision, 3D generation models, and Vision-Language foundation models.
            </p>
            <p align="center">
              <a href="mailto:lulutang978@gmail.com">Email</a> &nbsp|&nbsp
              <a href="https://github.com/Julie-tang00">GitHub</a> &nbsp|&nbsp
              <a href="https://scholar.google.com/citations?hl=en&user=o2fG4xUAAAAJ">Google Scholar</a>
            </p>
          </td>
          <td width="33%">
            <img src="image/tanglulu.jpg" width="250" alt="headshot">
          </td>
        </tr>
      </table>

      <!-- Research Section -->
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="100%" valign="middle">
            <heading>Selected Publications [todo]</heading>
            <p>(*: Equal Contribution, &clubs;: corresponding author)</p>
          </td>
        </tr>
      </table>

      <!-- Research Paper -->
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <img style="width:100%;max-width:100%" src="image/pointbert.png" alt="PointBERT">
          </td>
          <td width="75%" valign="center">
            <papertitle>Point-BERT: Pre-Training 3D Point Cloud Transformers with Masked Point Modeling</papertitle>
            <br>
            <a href="https://yuxumin.github.io/">Xumin Yu</a>*,
            <a href="https://julie-tang00.github.io/"><strong>Lulu Tang</strong></a>*,
            <a href="https://raoyongming.github.io/">Yongming Rao</a>*,
            <a href="http://www.ai.pku.edu.cn/info/1139/1243.htm">Tiejun Huang</a>,
            <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>,
            <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>&clubs;
            <br>
            <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022
            <br>
            <a href="https://arxiv.org/abs/2111.14819">[arXiv]</a>
            <a href="https://github.com/lulutang0608/Point-BERT">[Code]</a>
            <a href="https://point-bert.ivg-research.xyz/">[Project Page]</a>
            <a href="https://zhuanlan.zhihu.com/p/484336830">[Post]</a>
            <br>
            <p>Point-BERT is a new paradigm for learning Transformers in an unsupervised manner by generalizing the concept of BERT onto 3D point cloud data.</p>
          </td>
        </tr>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <img style="width:100%;max-width:100%" src="image/TAP.png" alt="TAP">
          </td>
          <td width="75%" valign="center">
            <papertitle>TAP: Tokenize Anything via Prompting</papertitle>
            <br>
            <a href="https://github.com/PhyscalX/">Ting Pan</a>*,
            <a href="https://julie-tang00.github.io/"><strong>Lulu Tang</strong></a>*,
            <a href="https://www.xloong.wang/">Xinlong Wang</a>&clubs;,
            <a href="https://scholar.google.com/citations?user=Vkzd7MIAAAAJ&hl=en">Shiguang Shan</a>,
            <br>
            <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2024
            <br>
            <a href="https://arxiv.org/pdf/2312.09128">[arXiv]</a>
            <a href="https://github.com/baaivision/tokenize-anything?tab=readme-ov-file">[Code]</a>
            <a href="https://huggingface.co/spaces/BAAI/tokenize-anything">[Demo]</a>
            <br>
            <p>TAP is a unified and promptable model capable of simultaneously segmenting, recognizing, and captioning arbitrary regions, with flexible visual prompts (point, box and sketch).</p>
          </td>
        </tr>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <img style="width:100%;max-width:100%" src="image/see3d.png" alt="see3d">
          </td>
          <td width="75%" valign="center">
            <papertitle>You See it, You Got it: Learning 3D Creation on Pose-Free Videos at Scale</papertitle>
            <br>
            <a href="https://mabaorui.github.io/">Baorui Ma</a>*,
            <a href="https://gaohchen.github.io/">Huachen Gao</a>*,
            <a href="https://github.com/Bitterdhg/">Haoge Deng</a>*,
            <a href="https://greatlog.github.io/">Zhengxiong Luo</a>,
            <a href="https://scholar.google.com/citations?user=knvEK4AAAAAJ&hl=en">Tiejun Huang</a>,
            <a href="https://julie-tang00.github.io/"> <strong>Lulu Tang</strong></a>&clubs;,
            <a href="https://www.xloong.wang/">Xinlong Wang</a>&clubs;,
            <br>
            <br>
            <a href="https://arxiv.org/abs/2412.06699">[arxiv]</a>
            <a href="https://vision.baai.ac.cn/see3d/">[Project page]</a>
            <a href="https://github.com/baaivision/See3D">[Code]</a>
            <a href="https://github.com/baaivision/See3D">[Dataset]</a>
            <a href="https://mp.weixin.qq.com/s/tP_YOkL6kAdeoHf-44Ls5Q">[Post]</a>
            <br>
            <p>See3D is a scalable visual-conditional MVD model for open-world 3D creation, which can be trained on web-scale video collections without camera pose annotations.</p>
          </td>
        </tr>
      </table>
      
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <img style="width:100%;max-width:100%" src="image/spike-t.png" alt="spike-t">
          </td>
          <td width="75%" valign="center">
            <papertitle>Spike Transformer: Monocular Depth Estimation for Spiking Camera</papertitle>
            <br>
            <a href="https://github.com/Leozhangjiyuan">Jiyuan Zhang</a>*,
            <a href="https://julie-tang00.github.io/"><strong>Lulu Tang</strong></a>*,
            <a href="https://yuzhaofei.github.io/">Zhaofei Yu</a>&clubs;,
            <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>,
            <a href="http://www.ai.pku.edu.cn/info/1139/1243.htm">Tiejun Huang</a>
            <br>
            <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2022
            <br>
            <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670034.pdf">[Paper]</a>
            <a href="https://github.com/Leozhangjiyuan/MDE-SpikingCamera">[Code]</a>
            <a href="https://drive.google.com/file/d/1_YMVTe-egeOKk16GMVBwjX8Sk3wdrd-E/view?usp=sharing">[Model]</a>
            <a href="https://pan.baidu.com/s/1hji5GnFH5Ke_nDt-1Q76rg">[Data]</a>
            <br>
            <p>Spike-T is a  Transformer-based network for learning spike data and estimating monocular depth from continuous spike streams.</p>
          </td>
        </tr>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <img style="width:100%;max-width:100%" src="image/pueva.png" alt="pueva">
          </td>
          <td width="75%" valign="center">
            <papertitle>An edge-vector based approximation solution for flexible-scale point cloud upsampling</papertitle>
            <br>
            <a href="https://github.com/GabrielleTse">Luqing Luo</a>*,
            <a href="https://julie-tang00.github.io/"><strong>Lulu Tang</strong></a>*,
            <a> Wanyi Zhou</a>,
            <a href="https://scholar.google.com.sg/citations?user=Flpe3S4AAAAJ&hl=en">Shizheng Wang</a>,
            <a href="https://scholar.google.com/citations?user=tlgIocMAAAAJ&hl=en">Zhi-Xin Yang&clubs;</a>
            <br>
            <em>IEEE/CVF International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2021
            <br>
            <a href="https://arxiv.org/abs/2204.10750">[arXiv]</a>
            <a href="https://github.com/GabrielleTse/PU-EVA?tab=readme-ov-file">[Code]</a>
            <br>
          </td>
        </tr>
      </table> 

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <img style="width:100%;max-width:100%" src="image/geo.png" alt="geo">
          </td>
          <td width="75%" valign="center">
            <papertitle>Improving Semantic Analysis on Point Clouds via Auxiliary Supervision of Local Geometric Priors</papertitle>
            <br>
            <a href="https://julie-tang00.github.io/"><strong>Lulu Tang</strong></a>*,
            <a href="https://scholar.google.com/citations?hl=en&user=pbNCoTwAAAAJ&view_op=list_works&sortby=pubdate"> Ke Chen</a>*,
            <a> Chaozheng Wu </a>,
            <a> Yu Hong </a>,
            <a href="https://scholar.google.com/citations?user=Mf9VHRcAAAAJ&hl=en">Kui Jia&clubs;</a>,           
            <a href="https://scholar.google.com/citations?user=tlgIocMAAAAJ&hl=en">Zhi-Xin Yang&clubs;</a>
            <br>
            <em>IEEE Transactions on Cybernetics</em>, 2020
            <br>
            <a href="https://ieeexplore.ieee.org/abstract/document/9238491">[arXiv]</a>
            <br>
          </td>
        </tr>
      </table> 
      
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <img style="width:100%;max-width:100%" src="image/CCA.png" alt="CCA">
          </td>
          <td width="75%" valign="center">
            <papertitle>Canonical correlation analysis regularization: An effective deep multiview learning baseline for RGB-D object recognition</papertitle>
            <br>
            <a href="https://julie-tang00.github.io/"><strong>Lulu Tang</strong></a>*,
            <a href="https://scholar.google.com/citations?user=tlgIocMAAAAJ&hl=en">Zhi-Xin Yang&clubs;</a>,
            <a href="https://scholar.google.com/citations?user=Mf9VHRcAAAAJ&hl=en">Kui Jia&clubs;</a>  
            <br>
            <em>IEEE Transactions on Cognitive and Developmental Systems</em>, 2019
            <br>
            <a href="https://ieeexplore.ieee.org/abstract/document/8452146">[Paper]</a>
            <br>
          </td>
        </tr>
        
      </table>
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <img style="width:100%;max-width:100%" src="image/PR.png" alt="PR">
          </td>
          <td width="75%" valign="center">
            <papertitle>Improving deep learning on point cloud by maximizing mutual information across layers</papertitle>
            <br>
            <a href="https://scholar.google.com/citations?user=FTFoAX8AAAAJ&hl=en">Di Wang</a>*,
            <a href="https://julie-tang00.github.io/"><strong>Lulu Tang</strong></a>,
            <a href="https://scholar.google.com/citations?user=r648SiYAAAAJ&hl=en">Xu Wang</a>, 
            <a href="https://github.com/GabrielleTse">Luqing Luo</a>,           
            <a href="https://scholar.google.com/citations?user=tlgIocMAAAAJ&hl=en">Zhi-Xin Yang&clubs;</a>
 
            <br>
            <em>Pattern Recognition</em>, 2022
            <br>
            <a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320322003739">[Paper]</a>
            <br>
          </td>
        </tr>
      </table>
      
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <img style="width:100%;max-width:100%" src="image/ELM.png" alt="ELM">
          </td>
          <td width="75%" valign="center">
            <papertitle>Multi-View CNN Feature Aggregation with ELM Auto-Encoder for 3D Shape Recognition</papertitle>
            <br>
            <a href="https://scholar.google.com/citations?user=tlgIocMAAAAJ&hl=en">Zhi-Xin Yang</a>*,
            <a href="https://julie-tang00.github.io/"><strong>Lulu Tang</strong> &clubs;</a>,
            <a>Kun Zhang</a>, 
            <a href="https://scholar.google.com.tw/citations?user=2DVvmmYAAAAJ&hl=zh-TW">Pak Kin Wong</a>           
            <br>
            <em>Cognitive Computation</em>, 2022
            <br>
            <a href="https://link.springer.com/article/10.1007/s12559-018-9598-1">[Paper]</a>
            <br>
          </td>
        </tr>
      </table>    
      
    </td>
  </tr>
</table>
</body>
</html>
